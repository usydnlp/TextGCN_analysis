{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "final code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K9dWTv5I07_"
      },
      "source": [
        "# Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yn4zQ7DI4T0"
      },
      "source": [
        "EDGE = 2 # 0:d2w 1:d2w+w2w 2:d2w+w2w+d2d\n",
        "NODE = 0 # 0:one-hot #1:BERT \n",
        "NUM_LAYERS = 2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJPI-IXrBkrP"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GyzNkI7W03D"
      },
      "source": [
        "original_train_sentences = \n",
        "original_labels_train = \n",
        "original_test_sentences = \n",
        "original_labels_test = \n",
        "\n",
        "train_size = len(original_train_sentences)\n",
        "test_size = len(original_test_sentences)\n",
        "sentences = original_train_sentences + original_test_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2W7wKTBfa71"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hobYcJ5OX5oT"
      },
      "source": [
        "## Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtWyhXiueMOq"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "unique_labels=np.unique(original_labels_train)\n",
        "\n",
        "num_class = len(unique_labels)\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(unique_labels)\n",
        "\n",
        "print(unique_labels)\n",
        "print(lEnc.transform(unique_labels))\n",
        "\n",
        "train_labels = lEnc.transform(original_labels_train)\n",
        "test_labels = lEnc.transform(original_labels_test)\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "labels = train_labels.tolist()+test_labels.tolist()\n",
        "labels = torch.LongTensor(labels).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMkEBxr6fMQi"
      },
      "source": [
        "## Remove Stopwords and less frequent words, tokenize sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xRG94uDfaBV"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "remove_limit = 5\n",
        "\n",
        "\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip().lower()\n",
        "\n",
        "original_word_freq = {}  # to remove rare words\n",
        "for sentence in sentences:\n",
        "    temp = clean_str(sentence)\n",
        "    word_list = temp.split()\n",
        "    for word in word_list:\n",
        "        if word in original_word_freq:\n",
        "            original_word_freq[word] += 1\n",
        "        else:\n",
        "            original_word_freq[word] = 1   \n",
        "\n",
        "tokenize_sentences = []\n",
        "word_list_dict = {}\n",
        "for sentence in sentences:\n",
        "    temp = clean_str(sentence)\n",
        "    word_list_temp = temp.split()\n",
        "    doc_words = []\n",
        "    for word in word_list_temp: \n",
        "        if word in original_word_freq and word not in stop_words and original_word_freq[word] >= remove_limit:\n",
        "            doc_words.append(word)\n",
        "            word_list_dict[word] = 1\n",
        "    tokenize_sentences.append(doc_words)\n",
        "word_list = list(word_list_dict.keys())\n",
        "vocab_length = len(word_list)\n",
        "\n",
        "#word to id dict\n",
        "word_id_map = {}\n",
        "for i in range(vocab_length):\n",
        "    word_id_map[word_list[i]] = i            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqLUncB2Pn_L"
      },
      "source": [
        "node_size = train_size + vocab_length + test_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0o8wcXgrTiD"
      },
      "source": [
        "# Model input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZbRV2wYxY1U"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znJ7Grz7fQ2L"
      },
      "source": [
        "## Build Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BSg1uNgV3_7"
      },
      "source": [
        "from math import log\n",
        "row = []\n",
        "col = []\n",
        "weight = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QESQPT88AqsI"
      },
      "source": [
        "### word-word: PMI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNlJoLFagXhv"
      },
      "source": [
        "if EDGE >= 1:\n",
        "    window_size = 20\n",
        "    total_W = 0\n",
        "    word_occurrence = {}\n",
        "    word_pair_occurrence = {}\n",
        "\n",
        "    def ordered_word_pair(a, b):\n",
        "        if a > b:\n",
        "            return b, a\n",
        "        else:\n",
        "            return a, b\n",
        "\n",
        "    def update_word_and_word_pair_occurrence(q):\n",
        "        unique_q = list(set(q))\n",
        "        for i in unique_q:\n",
        "            try:\n",
        "                word_occurrence[i] += 1\n",
        "            except:\n",
        "                word_occurrence[i] = 1\n",
        "        for i in range(len(unique_q)):\n",
        "            for j in range(i+1, len(unique_q)):\n",
        "                word1 = unique_q[i]\n",
        "                word2 = unique_q[j]\n",
        "                word1, word2 = ordered_word_pair(word1, word2)\n",
        "                try:\n",
        "                    word_pair_occurrence[(word1, word2)] += 1\n",
        "                except:\n",
        "                    word_pair_occurrence[(word1, word2)] = 1\n",
        "\n",
        "\n",
        "    for ind in tqdm(range(train_size+test_size)):\n",
        "        words = tokenize_sentences[ind]\n",
        "\n",
        "        q = []\n",
        "        # push the first (window_size) words into a queue\n",
        "        for i in range(min(window_size, len(words))):\n",
        "            q += [word_id_map[words[i]]]\n",
        "        # update the total number of the sliding windows\n",
        "        total_W += 1\n",
        "        # update the number of sliding windows that contain each word and word pair\n",
        "        update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "        now_next_word_index = window_size\n",
        "        # pop the first word out and let the next word in, keep doing this until the end of the document\n",
        "        while now_next_word_index<len(words):\n",
        "            q.pop(0)\n",
        "            q += [word_id_map[words[now_next_word_index]]]\n",
        "            now_next_word_index += 1\n",
        "            # update the total number of the sliding windows\n",
        "            total_W += 1\n",
        "            # update the number of sliding windows that contain each word and word pair\n",
        "            update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "    for word_pair in word_pair_occurrence:\n",
        "        i = word_pair[0]\n",
        "        j = word_pair[1]\n",
        "        count = word_pair_occurrence[word_pair]\n",
        "        word_freq_i = word_occurrence[i]\n",
        "        word_freq_j = word_occurrence[j]\n",
        "        pmi = log((count * total_W) / (word_freq_i * word_freq_j))\n",
        "        if pmi <=0:\n",
        "            continue\n",
        "        row.append(train_size + i)\n",
        "        col.append(train_size + j)\n",
        "        weight.append(pmi)\n",
        "        row.append(train_size + j)\n",
        "        col.append(train_size + i)\n",
        "        weight.append(pmi)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hynLnT3a33kW"
      },
      "source": [
        "### doc-word: Tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnSPqhg1lHps"
      },
      "source": [
        "#get each word appears in which document\n",
        "word_doc_list = {}\n",
        "for word in word_list:\n",
        "    word_doc_list[word]=[]\n",
        "\n",
        "for i in range(len(tokenize_sentences)):\n",
        "    doc_words = tokenize_sentences[i]\n",
        "    unique_words = set(doc_words)\n",
        "    for word in unique_words:\n",
        "        exsit_list = word_doc_list[word]\n",
        "        exsit_list.append(i)\n",
        "        word_doc_list[word] = exsit_list\n",
        "\n",
        "#document frequency\n",
        "word_doc_freq = {}\n",
        "for word, doc_list in word_doc_list.items():\n",
        "    word_doc_freq[word] = len(doc_list)\n",
        "\n",
        "# term frequency\n",
        "doc_word_freq = {}\n",
        "\n",
        "for doc_id in range(len(tokenize_sentences)):\n",
        "    words = tokenize_sentences[doc_id]\n",
        "    for word in words:\n",
        "        word_id = word_id_map[word]\n",
        "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
        "        if doc_word_str in doc_word_freq:\n",
        "            doc_word_freq[doc_word_str] += 1\n",
        "        else:\n",
        "            doc_word_freq[doc_word_str] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6elPPFO_sXp"
      },
      "source": [
        "for i in range(len(tokenize_sentences)):\n",
        "    words = tokenize_sentences[i]\n",
        "    doc_word_set = set()\n",
        "    for word in words:\n",
        "        if word in doc_word_set:\n",
        "            continue\n",
        "        j = word_id_map[word]\n",
        "        key = str(i) + ',' + str(j)\n",
        "        freq = doc_word_freq[key]\n",
        "        if i < train_size:\n",
        "            row.append(i)\n",
        "        else:\n",
        "            row.append(i + vocab_length)\n",
        "        col.append(train_size + j)\n",
        "        idf = log(1.0 * len(tokenize_sentences) / word_doc_freq[word_list[j]])\n",
        "        weight.append(freq * idf)\n",
        "        doc_word_set.add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAr6ygKhWTc-"
      },
      "source": [
        "### doc-doc: jaccard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4-EH15oWWSX"
      },
      "source": [
        "import nltk\n",
        "\n",
        "if EDGE>=2:\n",
        "    tokenize_sentences_set = [set(s) for s in tokenize_sentences]\n",
        "    jaccard_threshold = 0.2\n",
        "    for i in tqdm(range(len(tokenize_sentences))):\n",
        "        for j in range(i+1, len(tokenize_sentences)):\n",
        "            jaccard_w = 1 - nltk.jaccard_distance(tokenize_sentences_set[i], tokenize_sentences_set[j])\n",
        "            if jaccard_w > jaccard_threshold:\n",
        "                if i < train_size:\n",
        "                    row.append(i)\n",
        "                else:\n",
        "                    row.append(i + vocab_length)\n",
        "                if j < train_size:\n",
        "                    col.append(j)\n",
        "                else:\n",
        "                    col.append(vocab_length + j)\n",
        "                weight.append(jaccard_w)\n",
        "                if j < train_size:\n",
        "                    row.append(j)\n",
        "                else:\n",
        "                    row.append(j + vocab_length)\n",
        "                if i < train_size:\n",
        "                    col.append(i)\n",
        "                else:\n",
        "                    col.append(vocab_length + i)\n",
        "                weight.append(jaccard_w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIkGgB2aZDk7"
      },
      "source": [
        "### Adjacent matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0O1Ucdhod9a"
      },
      "source": [
        "import scipy.sparse as sp\n",
        "adj = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n",
        "\n",
        "# build symmetric adjacency matrix\n",
        "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivyuexATkQFW"
      },
      "source": [
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo(), d_inv_sqrt\n",
        "    \n",
        "adj, norm_item = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape).to(device)\n",
        "\n",
        "adj = sparse_mx_to_torch_sparse_tensor(adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMgbhTstMSUA"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP9dqCskOrXT"
      },
      "source": [
        "if NODE == 0:\n",
        "    features = np.arange(node_size)\n",
        "    features = torch.FloatTensor(features).to(device)\n",
        "else:\n",
        "    !pip install flair\n",
        "\n",
        "    from flair.embeddings import TransformerDocumentEmbeddings, TransformerWordEmbeddings\n",
        "    from flair.data import Sentence\n",
        "    doc_embedding = TransformerDocumentEmbeddings('bert-base-uncased', fine_tune=False)\n",
        "    word_embedding = TransformerWordEmbeddings('bert-base-uncased', layers='-1',subtoken_pooling=\"mean\")\n",
        "\n",
        "    sent_embs = []\n",
        "    word_embs = {}\n",
        "\n",
        "    for ind in tqdm(range(train_size+test_size)):\n",
        "        sent = tokenize_sentences[ind]\n",
        "        sentence = Sentence(\" \".join(sent[:512]),use_tokenizer=False)\n",
        "        doc_embedding.embed(sentence)\n",
        "        sent_embs.append(sentence.get_embedding().tolist())\n",
        "        words = Sentence(\" \".join(sent[:512]),use_tokenizer=False)\n",
        "        word_embedding.embed(words)\n",
        "        for token in words:\n",
        "            word = token.text\n",
        "            embedding = token.embedding.tolist()\n",
        "            if word not in word_embs:\n",
        "                word_embs[word] = embedding\n",
        "            else:\n",
        "                word_embs[word] = np.minimum(word_embs[word], embedding)\n",
        "\n",
        "    word_embs_list = []\n",
        "    for word in word_list:\n",
        "        word_embs_list.append(word_embs[word])\n",
        "\n",
        "    features = sent_embs[:train_size] + word_embs_list + sent_embs[train_size:]\n",
        "\n",
        "    import scipy.sparse as sp\n",
        "    def preprocess_features(features):\n",
        "        \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "        rowsum = np.array(features.sum(1))\n",
        "        r_inv = np.power(rowsum, -1).flatten()\n",
        "        r_inv[np.isinf(r_inv)] = 0.\n",
        "        r_mat_inv = sp.diags(r_inv)\n",
        "        features = r_mat_inv.dot(features)\n",
        "        return features\n",
        "\n",
        "    features = preprocess_features(sp.csr_matrix(features)).todense()\n",
        "    features = torch.FloatTensor(features).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdx6RrUvjbF0"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39Kj8NQujiDH"
      },
      "source": [
        "## GCN Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNVkA-h7b3sP"
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features,  drop_out = 0, activation=None, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.zeros(1, out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters(in_features, out_features)\n",
        "        self.dropout = torch.nn.Dropout(drop_out)\n",
        "        self.activation =  activation\n",
        "\n",
        "    def reset_parameters(self,in_features, out_features):\n",
        "        stdv = np.sqrt(6.0/(in_features+out_features))\n",
        "        # stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        # if self.bias is not None:\n",
        "        #     torch.nn.init.zeros_(self.bias)\n",
        "            # self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "    def forward(self, input, adj, feature_less = False):\n",
        "        if feature_less:\n",
        "            support = self.weight\n",
        "            support = self.dropout(support)\n",
        "        else:\n",
        "            input = self.dropout(input)\n",
        "            support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k57M4sz4s4Md"
      },
      "source": [
        "## GCN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ-ZQuMzs5tZ"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, n_layers = 2):\n",
        "        super(GCN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.gc_list = []\n",
        "        if n_layers >= 2:\n",
        "            self.gc1 = GraphConvolution(nfeat, nhid, dropout, activation = nn.ReLU())\n",
        "            self.gc_list = nn.ModuleList([GraphConvolution(nhid, nhid, dropout, activation = nn.ReLU()) for _ in range(self.n_layers-2)])\n",
        "            self.gcf = GraphConvolution(nhid, nclass, dropout)\n",
        "        else:\n",
        "            self.gc1 = GraphConvolution(nfeat, nclass, dropout)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        if self.n_layers>=2:\n",
        "            x = self.gc1(x, adj, feature_less = True)\n",
        "            for i in range(self.n_layers-2):\n",
        "                x = self.gc_list[i](x,adj)\n",
        "            x = self.gcf(x,adj)\n",
        "        else:\n",
        "            x = self.gc1(x, adj, feature_less = True)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmhOG1yG--Ji"
      },
      "source": [
        "def cal_accuracy(predictions,labels):\n",
        "    pred = torch.argmax(predictions,-1).cpu().tolist()\n",
        "    lab = labels.cpu().tolist()\n",
        "    cor = 0\n",
        "    for i in range(len(pred)):\n",
        "        if pred[i] == lab[i]:\n",
        "            cor += 1\n",
        "    return cor/len(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEE4JxeUthCb"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIxII4QoticA"
      },
      "source": [
        "## Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdNsgxMG-Wwu"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "HIDDEN_DIM = 200\n",
        "DROP_OUT = 0.5\n",
        "LR = 0.02\n",
        "WEIGHT_DECAY = 0\n",
        "EARLY_STOPPING = 10\n",
        "NUM_EPOCHS = 200\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T98r4qZuuFyn"
      },
      "source": [
        "## Training and Validating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv9br9pgGw9R"
      },
      "source": [
        "def generate_train_val(train_pro=0.9):\n",
        "    real_train_size = int(train_pro*train_size)\n",
        "    val_size = train_size-real_train_size\n",
        "\n",
        "    idx_train = np.random.choice(train_size, real_train_size,replace=False)\n",
        "    idx_train.sort()\n",
        "    idx_val = []\n",
        "    pointer = 0\n",
        "    for v in range(train_size):\n",
        "        if pointer<len(idx_train) and idx_train[pointer] == v:\n",
        "            pointer +=1\n",
        "        else:\n",
        "            idx_val.append(v)\n",
        "    idx_test = range(train_size+vocab_length, node_size)\n",
        "    return idx_train, idx_val, idx_test\n",
        "\n",
        "idx_train, idx_val, idx_test = generate_train_val()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC7u3Jn2uIu4"
      },
      "source": [
        "import time\n",
        "\n",
        "def train_model(show_result = True):\n",
        "    val_loss = []\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output= model(features, adj)\n",
        "        loss_train = criterion(output[idx_train], labels[idx_train])\n",
        "        acc_train = cal_accuracy(output[idx_train], labels[idx_train])\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "        loss_val = criterion(output[idx_val], labels[idx_val])\n",
        "        val_loss.append(loss_val.item())\n",
        "        acc_val = cal_accuracy(output[idx_val], labels[idx_val])\n",
        "        if show_result:\n",
        "            print(  'Epoch: {:04d}'.format(epoch+1),\n",
        "                    'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "                    'acc_train: {:.4f}'.format(acc_train),\n",
        "                    'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "                    'acc_val: {:.4f}'.format(acc_val),\n",
        "                    'time: {:.4f}s'.format(time.time() - t))\n",
        "        \n",
        "        if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n",
        "            if show_result:\n",
        "                print(\"Early Stopping...\")\n",
        "            break\n",
        "\n",
        "train_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQwlWq6dyYJm"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmPNukmk40gd"
      },
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    predictions = torch.argmax(output[idx_test],-1).cpu().tolist()\n",
        "    acc = accuracy_score(test_labels,predictions)\n",
        "    f11 = f1_score(test_labels,predictions, average='macro')\n",
        "    f12 = f1_score(test_labels,predictions, average = 'weighted')\n",
        "    return acc, f11, f12\n",
        "\n",
        "print(test())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOFsVlv4hTgc"
      },
      "source": [
        "# Test 10 times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydMqrCkehVPW"
      },
      "source": [
        "for t in range(10):\n",
        "    model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    idx_train, idx_val, idx_test = generate_train_val()\n",
        "    train_model(show_result=False)\n",
        "    acc, f11, f12 = test()\n",
        "    test_acc_list.append(acc)\n",
        "    test_f11_list.append(f11)\n",
        "    test_f12_list.append(f12)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\",np.round(np.mean(test_acc_list),4))\n",
        "print(\"Macro F1:\",np.round(np.mean(test_f11_list),4))\n",
        "print(\"Weighted F1:\",np.round(np.mean(test_f12_list),4))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}